{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import warnings \n",
    "warnings.filterwarnings(\"ignore\")\n",
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "sns.set(style=\"white\", color_codes=True)\n",
    "from sklearn import datasets\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn import preprocessing\n",
    "from sklearn.decomposition import PCA\n",
    "import sklearn.metrics as sm\n",
    "import numpy as np\n",
    "%matplotlib inline\n",
    "# load data from sklearn directly\n",
    "df = datasets.load_iris()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.target_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.feature_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "iris = pd.DataFrame(df.data, columns=df.feature_names)\n",
    "iris['target']=df.target\n",
    "\n",
    "df.target_names\n",
    "\n",
    "iris['species'] = iris['target'].map({0:df.target_names[0],1:df.target_names[1],2:df.target_names[2]})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iris.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iris[\"species\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iris.plot(kind=\"scatter\", x=\"sepal length (cm)\", y=\"sepal width (cm)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.jointplot(x=\"sepal length (cm)\", y=\"sepal width (cm)\", data=iris, size=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.FacetGrid(iris, hue=\"species\", size=5) \\\n",
    "   .map(plt.scatter, \"sepal length (cm)\", \"sepal width (cm)\")\\\n",
    "   .add_legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.boxplot(x=\"species\", y=\"sepal length (cm)\", data=iris)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ax = sns.boxplot(x=\"species\", y=\"sepal length (cm)\", data=iris)\n",
    "ax = sns.stripplot(x=\"species\", y=\"sepal length (cm)\", data=iris, jitter=True, edgecolor=\"gray\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.violinplot(x=\"species\", y=\"sepal length (cm)\", data=iris, size=6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.FacetGrid(iris, hue=\"species\", size=6) \\\n",
    "   .map(sns.kdeplot, \"sepal length (cm)\") \\\n",
    "   .add_legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "setosa =  iris.loc[iris.species == \"setosa\"]\n",
    "versicolor = iris.loc[iris.species == \"versicolor\"]\n",
    "virginica = iris.loc[iris.species == \"virginica\"]\n",
    "f, ax = plt.subplots(figsize=(8, 8))\n",
    "\n",
    "ax = sns.kdeplot(setosa[['sepal length (cm)','sepal width (cm)']], cmap=\"Greens\", shade=True, shade_lowest=False)\n",
    "ax = sns.kdeplot(versicolor[['sepal length (cm)','sepal width (cm)']], cmap=\"Reds\", shade=True, shade_lowest=False)\n",
    "ax = sns.kdeplot(virginica[['sepal length (cm)','sepal width (cm)']], cmap=\"Blues\", shade=True, shade_lowest=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.pairplot(iris.drop(\"target\", axis=1), hue=\"species\", size=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.pairplot(iris.drop(\"target\", axis=1), hue=\"species\", size=4, diag_kind=\"kde\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.pairplot(iris.drop(\"target\", axis=1), hue=\"species\", size=4, diag_kind=\"kde\",markers=[\"o\",\"s\",\"D\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.pairplot(iris.drop(\"target\", axis=1), hue=\"species\", size=4, diag_kind=\"kde\",kind = \"reg\", markers=[\"o\",\"s\",\"D\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iris.drop(\"target\", axis=1).boxplot(by=\"species\", figsize=(16, 9))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(1, figsize=(16, 9))\n",
    "from pandas.tools.plotting import andrews_curves\n",
    "andrews_curves(iris.drop(\"target\", axis=1), \"species\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(1, figsize=(16, 9))\n",
    "from pandas.tools.plotting import parallel_coordinates\n",
    "parallel_coordinates(iris.drop(\"target\", axis=1), \"species\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(1, figsize=(16, 9))\n",
    "from pandas.tools.plotting import radviz\n",
    "radviz(iris.drop(\"target\", axis=1), \"species\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(14,9))\n",
    "\n",
    "# Plot the training points - sepal\n",
    "plt.subplot(211)\n",
    "plt.scatter(iris[['sepal length (cm)']], iris[['sepal width (cm)']], c=iris['target'], cmap=plt.cm.coolwarm)\n",
    "plt.xlabel('Sepal length')\n",
    "plt.ylabel('Sepal width')\n",
    "plt.title('Sepal')\n",
    "\n",
    "# Plot the training points - pepal\n",
    "plt.subplot(212)\n",
    "plt.scatter(iris[['petal length (cm)']], iris[['petal width (cm)']], c=iris['target'], cmap=plt.cm.coolwarm)\n",
    "plt.xlabel('Petal length')\n",
    "plt.ylabel('Petal width')\n",
    "plt.title('Petal')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(1, figsize=(16, 9))\n",
    "ax = Axes3D(fig, elev=-150, azim=110)\n",
    "\n",
    "X_reduced = PCA(n_components=3).fit_transform(df.data)\n",
    "\n",
    "ax.scatter(X_reduced[:, 0], X_reduced[:, 1], X_reduced[:, 2], c=iris['target'],cmap=plt.cm.coolwarm)\n",
    "\n",
    "ax.set_title(\"First three PCA directions\")\n",
    "\n",
    "ax.set_xlabel(\"1st eigenvector\")\n",
    "ax.set_ylabel(\"2nd eigenvector\")\n",
    "ax.set_zlabel(\"3rd eigenvector\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df.data\n",
    "y = df.target\n",
    "\n",
    "target_names = df.target_names\n",
    "\n",
    "pca = PCA(n_components=2)\n",
    "X_r = pca.fit(X).transform(X)\n",
    "\n",
    "# Percentage of variance explained for each components\n",
    "print('explained variance ratio (first two components): %s'\n",
    "      % str(pca.explained_variance_ratio_))\n",
    "\n",
    "plt.figure(figsize=(16,9))\n",
    "colors = ['red', 'blue', 'grey']\n",
    "lw = 2\n",
    "for color, i, target_name in zip(colors, [0, 1, 2], target_names):\n",
    "    plt.scatter(X_r[y == i, 0], X_r[y == i, 1], color=color, alpha=.8, lw=lw,\n",
    "                label=target_name)\n",
    "plt.legend(loc='best', shadow=False, scatterpoints=1)\n",
    "plt.title('PCA of IRIS dataset')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "lda = LinearDiscriminantAnalysis(n_components=2)\n",
    "X_r2 = lda.fit(X, y).transform(X)\n",
    "plt.figure(figsize=(16,9))\n",
    "for color, i, target_name in zip(colors, [0, 1, 2], target_names):\n",
    "    plt.scatter(X_r2[y == i, 0], X_r2[y == i, 1], alpha=.8, color=color,\n",
    "                label=target_name)\n",
    "plt.legend(loc='best', shadow=False, scatterpoints=1)\n",
    "plt.title('LDA of IRIS dataset')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#K-means model \n",
    "\n",
    "model = KMeans(n_clusters=3)\n",
    "model.fit(df.data)\n",
    "\n",
    "plt.figure(figsize=(14,7))\n",
    "\n",
    "colormap = np.array(['red', 'blue', 'grey'])\n",
    "\n",
    "# Plot the Original Classifications\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.scatter(iris[['petal length (cm)']], iris[['petal width (cm)']], c=colormap[iris['target']],cmap=plt.cm.coolwarm,s=40)\n",
    "plt.title('Real Classification')\n",
    "\n",
    "# Plot the Models Classifications\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.scatter(iris[['petal length (cm)']], iris[['petal width (cm)']], c=colormap[model.labels_],cmap=plt.cm.coolwarm,s=40)\n",
    "plt.title('K Mean Classification')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = KMeans(n_clusters=3)\n",
    "model.fit(df.data)\n",
    "\n",
    "plt.figure(figsize=(14,7))\n",
    "\n",
    "colormap = np.array(['red', 'blue', 'grey'])\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.scatter(iris[['sepal length (cm)']], iris[['sepal width (cm)']], c=colormap[iris['target']],cmap=plt.cm.coolwarm,s=40)\n",
    "plt.title('Real Classification')\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.scatter(iris[['sepal length (cm)']], iris[['sepal width (cm)']], c=colormap[model.labels_],cmap=plt.cm.coolwarm,s=40)\n",
    "plt.title('K Mean Classification')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#emsemble method\n",
    "from matplotlib.colors import ListedColormap\n",
    "from sklearn import clone\n",
    "from sklearn.ensemble import (RandomForestClassifier, ExtraTreesClassifier, AdaBoostClassifier)\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "# Parameters\n",
    "n_classes = 3\n",
    "n_estimators = 30\n",
    "cmap = plt.cm.RdYlBu\n",
    "plot_step = 0.02  # fine step width for decision surface contours\n",
    "plot_step_coarser = 0.5  # step widths for coarse classifier guesses\n",
    "RANDOM_SEED = 13  # fix the seed on each iteration\n",
    "\n",
    "# Load data\n",
    "plot_idx = 1\n",
    "\n",
    "models = [DecisionTreeClassifier(max_depth=None),\n",
    "          RandomForestClassifier(n_estimators=n_estimators),\n",
    "          ExtraTreesClassifier(n_estimators=n_estimators),\n",
    "          AdaBoostClassifier(DecisionTreeClassifier(max_depth=3),\n",
    "                             n_estimators=n_estimators)]\n",
    "\n",
    "plt.figure(figsize=(16,9))\n",
    "for pair in ([0, 1], [0, 2], [2, 3]):\n",
    "    for model in models:\n",
    "        # We only take the two corresponding features\n",
    "        X = df.data[:, pair]\n",
    "        y = df.target\n",
    "\n",
    "        # Shuffle\n",
    "        idx = np.arange(X.shape[0])\n",
    "        np.random.seed(RANDOM_SEED)\n",
    "        np.random.shuffle(idx)\n",
    "        X = X[idx]\n",
    "        y = y[idx]\n",
    "\n",
    "        # Standardize\n",
    "        mean = X.mean(axis=0)\n",
    "        std = X.std(axis=0)\n",
    "        X = (X - mean) / std\n",
    "\n",
    "        # Train\n",
    "        clf = clone(model)\n",
    "        clf = model.fit(X, y)\n",
    "\n",
    "        scores = clf.score(X, y)\n",
    "        # Create a title for each column and the console by using str() and\n",
    "        # slicing away useless parts of the string\n",
    "        model_title = str(type(model)).split(\n",
    "            \".\")[-1][:-2][:-len(\"Classifier\")]\n",
    "\n",
    "        model_details = model_title\n",
    "        if hasattr(model, \"estimators_\"):\n",
    "            model_details += \" with {} estimators\".format(\n",
    "                len(model.estimators_))\n",
    "        print(model_details + \" with features\", pair,\n",
    "              \"has a score of\", scores)\n",
    "\n",
    "        plt.subplot(3, 4, plot_idx)\n",
    "        if plot_idx <= len(models):\n",
    "            # Add a title at the top of each column\n",
    "            plt.title(model_title)\n",
    "\n",
    "        # Now plot the decision boundary using a fine mesh as input to a\n",
    "        # filled contour plot\n",
    "        x_min, x_max = X[:, 0].min() - 1, X[:, 0].max() + 1\n",
    "        y_min, y_max = X[:, 1].min() - 1, X[:, 1].max() + 1\n",
    "        xx, yy = np.meshgrid(np.arange(x_min, x_max, plot_step),\n",
    "                             np.arange(y_min, y_max, plot_step))\n",
    "\n",
    "        # Plot either a single DecisionTreeClassifier or alpha blend the\n",
    "        # decision surfaces of the ensemble of classifiers\n",
    "        if isinstance(model, DecisionTreeClassifier):\n",
    "            Z = model.predict(np.c_[xx.ravel(), yy.ravel()])\n",
    "            Z = Z.reshape(xx.shape)\n",
    "            cs = plt.contourf(xx, yy, Z, cmap=cmap)\n",
    "        else:\n",
    "            # Choose alpha blend level with respect to the number\n",
    "            # of estimators\n",
    "            # that are in use (noting that AdaBoost can use fewer estimators\n",
    "            # than its maximum if it achieves a good enough fit early on)\n",
    "            estimator_alpha = 1.0 / len(model.estimators_)\n",
    "            for tree in model.estimators_:\n",
    "                Z = tree.predict(np.c_[xx.ravel(), yy.ravel()])\n",
    "                Z = Z.reshape(xx.shape)\n",
    "                cs = plt.contourf(xx, yy, Z, alpha=estimator_alpha, cmap=cmap)\n",
    "\n",
    "        # Build a coarser grid to plot a set of ensemble classifications\n",
    "        # to show how these are different to what we see in the decision\n",
    "        # surfaces. These points are regularly space and do not have a\n",
    "        # black outline\n",
    "        xx_coarser, yy_coarser = np.meshgrid(\n",
    "            np.arange(x_min, x_max, plot_step_coarser),\n",
    "            np.arange(y_min, y_max, plot_step_coarser))\n",
    "        Z_points_coarser = model.predict(np.c_[xx_coarser.ravel(),\n",
    "                                         yy_coarser.ravel()]\n",
    "                                         ).reshape(xx_coarser.shape)\n",
    "        cs_points = plt.scatter(xx_coarser, yy_coarser, s=15,\n",
    "                                c=Z_points_coarser, cmap=cmap,\n",
    "                                edgecolors=\"none\")\n",
    "\n",
    "        # Plot the training points, these are clustered together and have a\n",
    "        # black outline\n",
    "        plt.scatter(X[:, 0], X[:, 1], c=y,\n",
    "                    cmap=ListedColormap(['r', 'y', 'b']),\n",
    "                    edgecolor='k', s=20)\n",
    "        plot_idx += 1  # move on to the next plot in sequence\n",
    "\n",
    "plt.suptitle(\"Classifiers on feature subsets of the Iris dataset\")\n",
    "plt.axis(\"tight\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Gaussian process for machine learning\n",
    "from sklearn.gaussian_process import GaussianProcessClassifier\n",
    "from sklearn.gaussian_process.kernels import RBF\n",
    "\n",
    "# import some data to play with\n",
    "X = df.data[:, :2]  # we only take the first two features.\n",
    "y = np.array(iris.target, dtype=int)\n",
    "\n",
    "h = .02  # step size in the mesh\n",
    "\n",
    "kernel = 1.0 * RBF([1.0])\n",
    "gpc_rbf_isotropic = GaussianProcessClassifier(kernel=kernel).fit(X, y)\n",
    "kernel = 1.0 * RBF([1.0, 1.0])\n",
    "gpc_rbf_anisotropic = GaussianProcessClassifier(kernel=kernel).fit(X, y)\n",
    "\n",
    "# create a mesh to plot in\n",
    "x_min, x_max = X[:, 0].min() - 1, X[:, 0].max() + 1\n",
    "y_min, y_max = X[:, 1].min() - 1, X[:, 1].max() + 1\n",
    "xx, yy = np.meshgrid(np.arange(x_min, x_max, h),\n",
    "                     np.arange(y_min, y_max, h))\n",
    "\n",
    "titles = [\"Isotropic RBF\", \"Anisotropic RBF\"]\n",
    "plt.figure(figsize=(16, 9))\n",
    "for i, clf in enumerate((gpc_rbf_isotropic, gpc_rbf_anisotropic)):\n",
    "    # Plot the predicted probabilities. For that, we will assign a color to\n",
    "    # each point in the mesh [x_min, m_max]x[y_min, y_max].\n",
    "    plt.subplot(1, 2, i + 1)\n",
    "\n",
    "    Z = clf.predict_proba(np.c_[xx.ravel(), yy.ravel()])\n",
    "\n",
    "    # Put the result into a color plot\n",
    "    Z = Z.reshape((xx.shape[0], xx.shape[1], 3))\n",
    "    plt.imshow(Z, extent=(x_min, x_max, y_min, y_max), origin=\"lower\")\n",
    "\n",
    "    # Plot also the training points\n",
    "    plt.scatter(X[:, 0], X[:, 1], c=np.array([\"r\", \"g\", \"b\"])[y],\n",
    "                edgecolors=(0, 0, 0))\n",
    "    plt.xlabel('Sepal length')\n",
    "    plt.ylabel('Sepal width')\n",
    "    plt.xlim(xx.min(), xx.max())\n",
    "    plt.ylim(yy.min(), yy.max())\n",
    "    plt.xticks(())\n",
    "    plt.yticks(())\n",
    "    plt.title(\"%s, LML: %.3f\" %\n",
    "              (titles[i], clf.log_marginal_likelihood(clf.kernel_.theta)))\n",
    "\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Plot decision surface of multi-class SGD on iris dataset. \n",
    "#The hyperplanes corresponding to the three one-versus-all (OVA) classifiers are represented by the dashed lines.\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "plt.figure(figsize=(16, 9))\n",
    "# shuffle\n",
    "idx = np.arange(X.shape[0])\n",
    "np.random.seed(13)\n",
    "np.random.shuffle(idx)\n",
    "X = X[idx]\n",
    "y = y[idx]\n",
    "\n",
    "# standardize\n",
    "mean = X.mean(axis=0)\n",
    "std = X.std(axis=0)\n",
    "X = (X - mean) / std\n",
    "\n",
    "h = .02  # step size in the mesh\n",
    "\n",
    "clf = SGDClassifier(alpha=0.001).fit(X, y)\n",
    "\n",
    "# create a mesh to plot in\n",
    "x_min, x_max = X[:, 0].min() - 1, X[:, 0].max() + 1\n",
    "y_min, y_max = X[:, 1].min() - 1, X[:, 1].max() + 1\n",
    "xx, yy = np.meshgrid(np.arange(x_min, x_max, h),\n",
    "                     np.arange(y_min, y_max, h))\n",
    "\n",
    "# Plot the decision boundary. For that, we will assign a color to each\n",
    "# point in the mesh [x_min, x_max]x[y_min, y_max].\n",
    "Z = clf.predict(np.c_[xx.ravel(), yy.ravel()])\n",
    "# Put the result into a color plot\n",
    "Z = Z.reshape(xx.shape)\n",
    "cs = plt.contourf(xx, yy, Z, cmap=plt.cm.Paired)\n",
    "\n",
    "# Plot also the training points\n",
    "for i, color in zip(clf.classes_, colors):\n",
    "    idx = np.where(y == i)\n",
    "    plt.scatter(X[idx, 0], X[idx, 1], c=color, label=df.target_names[i],\n",
    "                cmap=plt.cm.Paired, edgecolor='black', s=20)\n",
    "plt.title(\"Decision surface of multi-class SGD\")\n",
    "plt.axis('tight')\n",
    "\n",
    "# Plot the three one-against-all classifiers\n",
    "xmin, xmax = plt.xlim()\n",
    "ymin, ymax = plt.ylim()\n",
    "coef = clf.coef_\n",
    "intercept = clf.intercept_\n",
    "\n",
    "\n",
    "def plot_hyperplane(c, color):\n",
    "    def line(x0):\n",
    "        return (-(x0 * coef[c, 0]) - intercept[c]) / coef[c, 1]\n",
    "\n",
    "    plt.plot([xmin, xmax], [line(xmin), line(xmax)],\n",
    "             ls=\"--\", color=color)\n",
    "\n",
    "\n",
    "for i, color in zip(clf.classes_, colors):\n",
    "    plot_hyperplane(i, color)\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Decision boundary of label propagation versus SVM on the Iris dataset\n",
    "from sklearn import svm\n",
    "from sklearn.semi_supervised import label_propagation\n",
    "plt.figure(figsize=(16, 9))\n",
    "\n",
    "rng = np.random.RandomState(0)\n",
    "\n",
    "# step size in the mesh\n",
    "h = .02\n",
    "\n",
    "y_30 = np.copy(y)\n",
    "y_30[rng.rand(len(y)) < 0.3] = -1\n",
    "y_50 = np.copy(y)\n",
    "y_50[rng.rand(len(y)) < 0.5] = -1\n",
    "\n",
    "# we create an instance of SVM and fit out data. We do not scale our\n",
    "# data since we want to plot the support vectors\n",
    "ls30 = (label_propagation.LabelSpreading().fit(X, y_30),\n",
    "        y_30)\n",
    "ls50 = (label_propagation.LabelSpreading().fit(X, y_50),\n",
    "        y_50)\n",
    "ls100 = (label_propagation.LabelSpreading().fit(X, y), y)\n",
    "rbf_svc = (svm.SVC(kernel='rbf').fit(X, y), y)\n",
    "\n",
    "# create a mesh to plot in\n",
    "x_min, x_max = X[:, 0].min() - 1, X[:, 0].max() + 1\n",
    "y_min, y_max = X[:, 1].min() - 1, X[:, 1].max() + 1\n",
    "xx, yy = np.meshgrid(np.arange(x_min, x_max, h), np.arange(y_min, y_max, h))\n",
    "\n",
    "# title for the plots\n",
    "titles = ['Label Spreading 30% data',\n",
    "          'Label Spreading 50% data',\n",
    "          'Label Spreading 100% data',\n",
    "          'SVC with rbf kernel']\n",
    "\n",
    "color_map = {-1: (1, 1, 1), 0: (0, 0, .9), 1: (1, 0, 0), 2: (.8, .6, 0)}\n",
    "\n",
    "for i, (clf, y_train) in enumerate((ls30, ls50, ls100, rbf_svc)):\n",
    "    # Plot the decision boundary. For that, we will assign a color to each\n",
    "    # point in the mesh [x_min, x_max]x[y_min, y_max].\n",
    "    plt.subplot(2, 2, i + 1)\n",
    "    Z = clf.predict(np.c_[xx.ravel(), yy.ravel()])\n",
    "\n",
    "    # Put the result into a color plot\n",
    "    Z = Z.reshape(xx.shape)\n",
    "    plt.contourf(xx, yy, Z, cmap=plt.cm.Paired)\n",
    "    plt.axis('off')\n",
    "\n",
    "    # Plot also the training points\n",
    "    colors = [color_map[y] for y in y_train]\n",
    "    plt.scatter(X[:, 0], X[:, 1], c=colors, edgecolors='black')\n",
    "    plt.title(titles[i])\n",
    "\n",
    "plt.suptitle(\"Unlabeled points are colored white\", y=0.1)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import svm\n",
    "\n",
    "#Create a mesh of points to plot in\n",
    "def make_meshgrid(x, y, h=.02):\n",
    "    x_min, x_max = x.min() - 1, x.max() + 1\n",
    "    y_min, y_max = y.min() - 1, y.max() + 1\n",
    "    xx, yy = np.meshgrid(np.arange(x_min, x_max, h),\n",
    "                         np.arange(y_min, y_max, h))\n",
    "    return xx, yy\n",
    "\n",
    "#Plot the decision boundaries for a classifier.\n",
    "def plot_contours(ax, clf, xx, yy, **params):\n",
    "    Z = clf.predict(np.c_[xx.ravel(), yy.ravel()])\n",
    "    Z = Z.reshape(xx.shape)\n",
    "    out = ax.contourf(xx, yy, Z, **params)\n",
    "    return out\n",
    "\n",
    "# we create an instance of SVM and fit out data. We do not scale our\n",
    "# data since we want to plot the support vectors\n",
    "C = 1.0  # SVM regularization parameter\n",
    "models = (svm.SVC(kernel='linear', C=C),\n",
    "          svm.LinearSVC(C=C),\n",
    "          svm.SVC(kernel='rbf', gamma=0.7, C=C),\n",
    "          svm.SVC(kernel='poly', degree=3, C=C))\n",
    "models = (clf.fit(X, y) for clf in models)\n",
    "\n",
    "# title for the plots\n",
    "titles = ('SVC with linear kernel',\n",
    "          'LinearSVC (linear kernel)',\n",
    "          'SVC with RBF kernel',\n",
    "          'SVC with polynomial (degree 3) kernel')\n",
    "\n",
    "# Set-up 2x2 grid for plotting.\n",
    "plt.figure(figsize=(16, 9))\n",
    "fig, sub = plt.subplots(2, 2)\n",
    "\n",
    "X0, X1 = X[:, 0], X[:, 1]\n",
    "xx, yy = make_meshgrid(X0, X1)\n",
    "\n",
    "for clf, title, ax in zip(models, titles, sub.flatten()):\n",
    "    plot_contours(ax, clf, xx, yy,\n",
    "                  cmap=plt.cm.coolwarm, alpha=0.8)\n",
    "    ax.scatter(X0, X1, c=y, cmap=plt.cm.coolwarm, s=20, edgecolors='k')\n",
    "    ax.set_xlim(xx.min(), xx.max())\n",
    "    ax.set_ylim(yy.min(), yy.max())\n",
    "    ax.set_xlabel('Sepal length')\n",
    "    ax.set_ylabel('Sepal width')\n",
    "    ax.set_xticks(())\n",
    "    ax.set_yticks(())\n",
    "    ax.set_title(title)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
